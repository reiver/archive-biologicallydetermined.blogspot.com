<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta charset="utf-8" />
<title>Tiny Nails</title>
</head>
<body>



<main>
<article>
	<hgroup>
		<h1>Tiny Nails</h1>
		<p><small>Incentive structures for scientists are interfering with scientific output</small></p>
		<p><small>2023-06-07</small></p>
	</hgroup>
	<section>
		<p>
			Goodhart's Law: ' When a measure becomes a target, it ceases to be a good measure'. Universities optimise for grades instead of knowledge. Politicians seek popularity, not the public good. Tomatoes are bred into heavy, flavorless sacks of water. Soviet Nail factories, when instructed to produce a certain number of nails per month, produced tiny, useless nails. Science is no different.
		</p>
		<p>
			Science began small and informal but is now performed on an industrial scale. Industrialisation requires quantification and targets, so during the 20th century, funding bodies began to assess scientists based on metrics such as their citation index – the number of times their work has been cited by others <a href="#footnote-1">[1]</a>. Today, scientific achievement is synonymous with publication in the most cited journals – an aspiring scientist must ‘publish or perish’.
		</p>
		<p>
			Imagine you are trying to maximise your publication rate and citations without any regard for its utility to other scientists or the public. How do you go about it? What are the scientific equivalents of tiny nails?
		</p>
		<p>
			Fraud is the obvious answer and fraud is indeed on the rise <a href="#footnote-2">[2]</a>. A close second to straight up fraud is selective publishing - perform many studies and report only the positive results. Here, statistics provide a host of ways to 'massage the data', particularly in fields such as biology and the social sciences where one's peers often lack mathematical expertise.
		</p>
		<p>
			The majority of scientists, however, are too scrupulous or cautious for the above. How else can you optimise your publication record for maximising citations? One way is to produce 'minimum publishable units' - the smallest measurable quanta of information acceptable to a journal. You can exaggerate the importance and/or novelty of each MPU by exploiting tenuous links to human disease, neglecting relevant prior research, and good old-fashioned hyperbole. You can also exaggerate the certainty of your conclusions by not performing replicate experiments, or experiments that might disprove your hypothesis. Finally, you can increase your citations by citing yourself and your friends whenever possible.
		</p>
		<p>
			Now, as people go, scientists are a fairly principled bunch and most try to avoid these practices as much as they can. However, Goodhart's law is deeply embedded in the system <a href="#footnote-3">[3]</a>. PhD students must publish, and quickly, to be competitive. Senior scientists are in a constant battle for funding and job security. Even those with tenure employ scientists on short-term contracts, who need publications, and quickly, for their next grant. All of these factors drive the desire to publish ‘tiny nails’. When biological research is proving to be so profoundly unreliable to the private sector <a href="#footnote-4">[4]</a>, something is very wrong.
		</p>
		<p>
			It has been suggested that the scientific literature is 'self-correcting' - that fraud or lax experimentation gets discovered eventually. But science is becoming more and more expensive, and replication is becoming increasingly difficult to perform. Correction may or may not take place, but in the meantime, the public's faith in science has been eroded, and with it any and all benefits that science brings. We can do better. The engine of knowledge could run more smoothly. So what ought to be done about it?
		</p>
		<p>
			Biologically Determined has a number of suggestions. We will resist the temptation to exaggerate their novelty.
		</p>
		<p>
			Firstly, shallow measures such as the number of publications or their citations index need to be dropped, or at least relied on less. This will require a shift in the culture of science, and is to a large extent already underway. Journals like PLoS One are providing a valuable service by publishing articles regardless of their novelty. Systems should be organised to publish negative results and replications, which should be taken into account when evaluating researcher performance.
		</p>
		<p>
			The emphasis on short-term contracts and career pressure in science should also be removed. Private sector firms like Google or Valve who require creative intellectual output have listened to the scientific research on the matter - a stressed brain produces tiny nails <a href="#footnote-5">[5]</a>. Science should do the same by providing more long-term contracts, and reforming the current system <a href="#footnote-6">[6]</a> in which huge numbers of PhD students are trained, providing cheap labour before leaving the field, disappointed and embittered. This would have the side benefit of efficiency - repeatedly training fresh PhD students in the same techniques is inefficient, and discourages streamlining and automation of protocols. It would also allow scientists to perform innovative, and therefore risky, experiments that the current system discourages.
		</p>
		<p>
			Finally, and probably most importantly, studies need to be reproduced in an organised manner. Several organisations exist or have been proposed <a href="#footnote-7">[7]</a> to accomplish just this. Ultimately, it will be necessary for us to accept that the stamp of 'peer reviewed' cannot, and does not, amount to a vote of absolute certainty. Scientists make sophisticated judgment calls in evaluating peer-reviewed evidence, and it is these judgments that need to be communicated to the public. Organised reproduction of papers would provide a further level of confidence.
		</p>
		<p>
			Different fields tend to have their own standards of certainty. Physics for instance, has much stricter statistical standards than the soft sciences, while fields like psychology are plagued by retractions and low confidence results <a href="#footnote-8">[8]</a>. While reproducibility will always be easier to achieve in the ‘harder’ sciences, this should mean that higher, not lower standards are held to in the soft sciences. Fields such as dietary science, and psychology, that offer directly actionable advice to the public, should be less willing to offer shaky conclusions. The media’s reporting of such studies will always be imperfect, and scientists must act with this in mind.
		</p>
		<p>
			Much of this is accomplishable from within science. But it is dangerous to assume that science can be entirely self-regulating. Scientists, especially those at the top, are quite conservative about its structure. For instance, in recent years governments have begun to demand publication in open access journals. Individual researchers, who would otherwise grumble about the system but still publish behind ridiculous pay walls, must no longer do so or risk losing funding. A change made possible only by outside regulation.
		</p>
		<p>
			Science is in many ways the last remaining sacred cow of our age. And in many ways this is justified – scientists are not, after all, in it for the cars or money, nor are they afforded particularly high social status. But scientists are still human beings and as susceptible to incentive structures as the rest of us. The public bodies that fund scientific research must ensure that the scientific market is properly regulated. It is after all, their money.
		</p>
	</section>
	<section>
		<p id="footnote-1">
			1 – Hirsch index. The most commonly scientific output measure
			<br><br />
			http://en.wikipedia.org/wiki/H-index
		</p>
		<p id="footnote-2">
			2 – Fraud on the Rise
			<br /><br />
			http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0005738
			<br /><br />
			http://www.pnas.org/content/109/42/17028
			<br /><br />
			http://www.nature.com/news/2011/111005/full/478026a.html
		</p>
		<p id="footnote-3">
			3 – Poor practice in Science
			<br /><br />
			http://www.nature.com/nature/journal/v483/n7391/full/483509a.html
			<br /><br />
			http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0010068
		</p>
		<p id="footnote-4">
			4 – Lack of reproducibility of biomedical results
			<br /><br />
			http://www.nature.com/nature/journal/v483/n7391/full/483531a.html
		</p>
		<p id="footnote-5">
			5- Creativity in the Private Sector
			<br /><br />
			http://m.garrettamini.com/2011/08/lessons-from-valve-how-to-build-a-designers-paradise/http://www.nytimes.com/2013/03/16/business/at-google-a-place-to-work-and-play.html?pagewanted=all
		</p>
		<p id="footnote-6">
			6 – economist
			<br /><br />
			http://www.economist.com/node/17723223
		</p>
		<p id="footnote-7">
			7- Organizations publishing replications, negative results
			<br /><br />
			http://www.jnrbm.com/
			<br /><br />
			http://www.plosone.org/
			<br /><br />
			http://www.alltrials.net/
		</p>
		<p id="footnote-8">
			8 – Retractions in Psychology
			<br /><br />
			http://www.nature.com/news/replication-studies-bad-copy-1.10634
			<br /><br />
			http://blogs.nature.com/news/2011/12/stapel.html
			<br /><br />
			http://blogs.nature.com/news/2011/12/stapel.html
		</p>
	</section>
</article>
</main>



</body>
</html>
